{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MapReduce_BoW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**2° PARCIAL - MapReduce BOW**\n",
        "---\n",
        "```\n",
        "Universidad Nacional de San Antonio Abad del Cusco\n",
        "Asignatura: Mineria de Datos\n",
        "Docente   : Carlos Fernando Montoya Cubas\n",
        "Autor     : Etson Ronaldao Rojas Cahuana\n",
        "Fecha     : 09/01/2022\n",
        "Lugar     : Cusco, Perú\n",
        "Proposito : Aplicar MapReduce BOW.\n",
        "```\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7a2w8dwQgZEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instalacion de pySpark en Google Colaboratory"
      ],
      "metadata": {
        "id": "oLTm7TP18_Tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHcj-zXMYpl-",
        "outputId": "d86e74f9-8bbd-48ef-be60-a4194245efbe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.0.1\n",
            "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2 MB 36 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 23.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612246 sha256=68a09569485e0a1600f81eb9dbb616b1c137fe3539e46cd12bd67f74f36e499b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inicializar variables de entorno"
      ],
      "metadata": {
        "id": "DrzRL-2-9A-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "XPOGH-esYk1x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mostrar version de entornos spark"
      ],
      "metadata": {
        "id": "AqYwJNo7-NL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "V4L86dDpZJDW",
        "outputId": "b74aa4ee-7767-49dc-ea2a-6de6de2540dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://2cf029c672e6:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Bag Of Words (Bow)**\n",
        "\n",
        "Es un método que se utiliza en el procesado del lenguaje para representar documentos ignorando el orden de las palabras. En este modelo, cada documento parece una bolsa que contiene algunas palabras. Este método permite un modelado de las palabras basado en diccionarios, donde cada bolsa contiene unas cuantas palabras del diccionario. Las principales ventajas de utilizar este modelo es su facilidad de uso y su eficiencia computacional.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cd3oHI3q62wE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQZVpQloVfok",
        "outputId": "8234430b-beb1-4618-c0b5-9a74a6fc2cf8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((1, 'Los'), 1),\n",
              " ((1, 'nuestros'), 1),\n",
              " ((1, 'únicos'), 1),\n",
              " ((1, 'enemigos'), 1),\n",
              " ((2, 'predecir'), 1),\n",
              " ((2, 'el'), 1),\n",
              " ((2, 'cada'), 1),\n",
              " ((2, 'decisión'), 2),\n",
              " ((2, 'que'), 1),\n",
              " ((2, 'tomas'), 1),\n",
              " ((2, 'significado'), 1),\n",
              " ((2, 'para'), 1),\n",
              " ((2, 'próxima'), 1),\n",
              " ((3, 'Parecía'), 1),\n",
              " ((3, 'un'), 1),\n",
              " ((3, 'profundo,'), 1),\n",
              " ((3, 'sigues'), 1),\n",
              " ((3, 'despierto'), 1),\n",
              " ((1, 'titanes'), 1),\n",
              " ((1, 'no'), 1),\n",
              " ((1, 'son'), 1),\n",
              " ((2, 'Nadie'), 1),\n",
              " ((2, 'puede'), 1),\n",
              " ((2, 'resultado,'), 1),\n",
              " ((2, 'tiene'), 1),\n",
              " ((2, 'un'), 1),\n",
              " ((2, 'sólo'), 1),\n",
              " ((2, 'afectar'), 1),\n",
              " ((2, 'tu'), 1),\n",
              " ((3, 'que'), 2),\n",
              " ((3, 'tenias'), 1),\n",
              " ((3, 'sueño'), 1),\n",
              " ((3, 'pero'), 1),\n",
              " ((3, 'parece'), 1),\n",
              " ((3, 'soñando'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import math\n",
        "data=[(1,'Los titanes no son nuestros únicos enemigos'),\n",
        "      (2,\"Nadie puede predecir el resultado, cada decisión que tomas tiene un significado sólo para afectar tu próxima decisión\"),\n",
        "      (3,\"Parecía que tenias un sueño profundo, pero parece que sigues soñando despierto\")]\n",
        "#creacion para el tokenizer o bag of words\n",
        "lines=sc.parallelize(data)\n",
        "lines.collect()\n",
        "map1=lines.flatMap(lambda x: [((x[0],i),1) for i in x[1].split()])\n",
        "map1.collect()\n",
        "#utilizacion del map reduce\n",
        "reduce=map1.reduceByKey(lambda x,y:x+y)\n",
        "#imprimir el baw of word de las 3 oraciones\n",
        "reduce.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Aplicacion TF- IDF**"
      ],
      "metadata": {
        "id": "DVBBLjngs0CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculamos TF\n",
        "tf=reduce.map(lambda x: (x[0][1],(x[0][0],x[1])))\n",
        "tf.collect()\n",
        "map3=reduce.map(lambda x: (x[0][1],(x[0][0],x[1],1)))\n",
        "map3.collect()\n",
        "map4=map3.map(lambda x:(x[0],x[1][2]))\n",
        "map4.collect()\n",
        "reduce2=map4.reduceByKey(lambda x,y:x+y)\n",
        "reduce2.collect()\n",
        "#Calculamos IDF\n",
        "idf=reduce2.map(lambda x: (x[0],math.log10(len(data)/x[1])))\n",
        "idf.collect()\n",
        "rdd=tf.join(idf)\n",
        "rdd.collect()\n",
        "#calculamos TF-IDF\n",
        "rdd=rdd.map(lambda x: (x[1][0][0],(x[0],x[1][0][1],x[1][1],x[1][0][1]*x[1][1]))).sortByKey()"
      ],
      "metadata": {
        "id": "Hja6kaUwrk1K"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Mostrar Tabla TF-IDF**"
      ],
      "metadata": {
        "id": "0Zuc0wEB-aID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "park = SparkSession(sc)\n",
        "rdd=rdd.map(lambda x: (x[0],x[1][0],x[1][1],x[1][2]))\n",
        "hasattr(rdd, \"toDF\")\n",
        "rdd.toDF([\"ID\",\"Token\",\"Bag_of_Word\",\"TF-IDF\"]).show(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycwIbQDJtan4",
        "outputId": "df20b61b-ec0c-441f-de6c-dac5dda54b2d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+-----------+-------------------+\n",
            "| ID|      Token|Bag_of_Word|             TF-IDF|\n",
            "+---+-----------+-----------+-------------------+\n",
            "|  1|    titanes|          1|0.47712125471966244|\n",
            "|  1|         no|          1|0.47712125471966244|\n",
            "|  1|        son|          1|0.47712125471966244|\n",
            "|  1|        Los|          1|0.47712125471966244|\n",
            "|  1|   nuestros|          1|0.47712125471966244|\n",
            "|  1|     únicos|          1|0.47712125471966244|\n",
            "|  1|   enemigos|          1|0.47712125471966244|\n",
            "|  2|   predecir|          1|0.47712125471966244|\n",
            "|  2|         el|          1|0.47712125471966244|\n",
            "|  2|      tomas|          1|0.47712125471966244|\n",
            "|  2|significado|          1|0.47712125471966244|\n",
            "|  2|      puede|          1|0.47712125471966244|\n",
            "|  2| resultado,|          1|0.47712125471966244|\n",
            "|  2|      tiene|          1|0.47712125471966244|\n",
            "|  2|         tu|          1|0.47712125471966244|\n",
            "|  2|       cada|          1|0.47712125471966244|\n",
            "|  2|   decisión|          2|0.47712125471966244|\n",
            "|  2|        que|          1|0.17609125905568124|\n",
            "|  2|       para|          1|0.47712125471966244|\n",
            "|  2|    próxima|          1|0.47712125471966244|\n",
            "|  2|         un|          1|0.17609125905568124|\n",
            "|  2|      Nadie|          1|0.47712125471966244|\n",
            "|  2|       sólo|          1|0.47712125471966244|\n",
            "|  2|    afectar|          1|0.47712125471966244|\n",
            "|  3|     parece|          1|0.47712125471966244|\n",
            "|  3|  despierto|          1|0.47712125471966244|\n",
            "|  3|        que|          2|0.17609125905568124|\n",
            "|  3|     tenias|          1|0.47712125471966244|\n",
            "|  3|      sueño|          1|0.47712125471966244|\n",
            "|  3|       pero|          1|0.47712125471966244|\n",
            "|  3|    soñando|          1|0.47712125471966244|\n",
            "|  3|    Parecía|          1|0.47712125471966244|\n",
            "|  3|         un|          1|0.17609125905568124|\n",
            "|  3|  profundo,|          1|0.47712125471966244|\n",
            "|  3|     sigues|          1|0.47712125471966244|\n",
            "+---+-----------+-----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}