{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocesamiento_5_ejercicios.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**2° PARCIAL - 5 ALGORITMOS DE PREPROCESAMIENTO**\n",
        "---\n",
        "```\n",
        "Universidad Nacional de San Antonio Abad del Cusco\n",
        "Asignatura: Mineria de Datos\n",
        "Docente   : Carlos Fernando Montoya Cubas\n",
        "Autor     : Etson Ronaldao Rojas Cahuana\n",
        "Fecha     : 09/01/2022\n",
        "Lugar     : Cusco, Perú\n",
        "Proposito : Implementar 5 algoritmos de preprocesamiento en pySpark.\n",
        "```\n",
        "---\n"
      ],
      "metadata": {
        "id": "7a2w8dwQgZEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instalacion de pySpark en Google Colaboratory"
      ],
      "metadata": {
        "id": "aNJPfC6LeKYB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyXuMq7esei0",
        "outputId": "5a397d63-6b4c-4dd7-dead-8a6e308e371e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.0.1\n",
            "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 51.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612246 sha256=c62e6a2ef176ce8727c72175d5c4a276e0f5362360eed5d9b5f11896cd821360\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "RPpeoDhAtR2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Escalonamiento**\n",
        "\n",
        "Los algoritmos basados en metodos de gradiente tienden a beneficiarse cuando los atributos estan entre $[0,1]$.\n",
        "\n",
        "$X_{i,j} = \\frac{X_{i,j} - max X_j} {max X_j - min_{X_j}}$\n",
        "\n"
      ],
      "metadata": {
        "id": "OdOEKE-p33SI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from pyspark import SparkConf \n",
        "from pyspark.context import SparkContext \n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType"
      ],
      "metadata": {
        "id": "9z6UXnys3C-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "03QuEY9f6Hjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos datos\n",
        "df = spark.createDataFrame([ (1, 'Aldo',12560,45,\"True\"),\n",
        "                             (2, 'Benzema',42560,90,\"False\"),\n",
        "                             (3, 'Cesar',31285,81,\"True\"),\n",
        "                             (4, 'Doris',25285,50,\"True\"),\n",
        "                             (5, 'Etson',32285,55,\"False\"),\n",
        "                             (6, 'Fausto',18085,61,\"True\"),\n",
        "                             (7, 'Gilberto',52185,70,\"False\"),\n",
        "                             (8, 'Humberto',10345,46,\"False\")\n",
        "                           ], [\"id_usuario\", \"Nombre\",\"Ingresos\",\"Por_dia\",\"Morocidad\"])\n",
        "\n",
        "#Mostrar datos iniciales\n",
        "print(\"Data :\")\n",
        "df.show(8)\n",
        "\n",
        "#UDF para convertir el tipo de columna de vector a tipo doble\n",
        "unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
        "\n",
        "#Iterando sobre columnas para el escalonamiento\n",
        "for i in [\"Ingresos\",\"Por_dia\"]:\n",
        "    #Transformación con VectorAssembler (conversión de columna a tipo vector)\n",
        "    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
        "\n",
        "    #Transformación MinMaxScaler\n",
        "    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Escalonado\")\n",
        "\n",
        "    #Canalización de VectorAssembler y MinMaxScaler\n",
        "    pipeline = Pipeline(stages=[assembler, scaler])\n",
        "\n",
        "    #Ajuste del pipeline en el marco de datos\n",
        "    df = pipeline.fit(df).transform(df).withColumn(i+\"_Escalonado\", unlist(i+\"_Escalonado\")).drop(i+\"_Vect\")\n",
        "\n",
        "print(\"Data + Escalonamiento :\")\n",
        "df.show(8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIUhQuJi4JTD",
        "outputId": "004d3667-4e17-446c-a0ee-1a74cbba907f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data :\n",
            "+----------+--------+--------+-------+---------+\n",
            "|id_usuario|  Nombre|Ingresos|Por_dia|Morocidad|\n",
            "+----------+--------+--------+-------+---------+\n",
            "|         1|    Aldo|   12560|     45|     True|\n",
            "|         2| Benzema|   42560|     90|    False|\n",
            "|         3|   Cesar|   31285|     81|     True|\n",
            "|         4|   Doris|   25285|     50|     True|\n",
            "|         5|   Etson|   32285|     55|    False|\n",
            "|         6|  Fausto|   18085|     61|     True|\n",
            "|         7|Gilberto|   52185|     70|    False|\n",
            "|         8|Humberto|   10345|     46|    False|\n",
            "+----------+--------+--------+-------+---------+\n",
            "\n",
            "Data + Escalonamiento :\n",
            "+----------+--------+--------+-------+---------+-------------------+------------------+\n",
            "|id_usuario|  Nombre|Ingresos|Por_dia|Morocidad|Ingresos_Escalonado|Por_dia_Escalonado|\n",
            "+----------+--------+--------+-------+---------+-------------------+------------------+\n",
            "|         1|    Aldo|   12560|     45|     True|              0.053|               0.0|\n",
            "|         2| Benzema|   42560|     90|    False|               0.77|               1.0|\n",
            "|         3|   Cesar|   31285|     81|     True|                0.5|               0.8|\n",
            "|         4|   Doris|   25285|     50|     True|              0.357|             0.111|\n",
            "|         5|   Etson|   32285|     55|    False|              0.524|             0.222|\n",
            "|         6|  Fausto|   18085|     61|     True|              0.185|             0.356|\n",
            "|         7|Gilberto|   52185|     70|    False|                1.0|             0.556|\n",
            "|         8|Humberto|   10345|     46|    False|                0.0|             0.022|\n",
            "+----------+--------+--------+-------+---------+-------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El valor minimo de la columna *Ingresos* es 10345 y el maximo es 52185, estos estan representados correctamente en el escalonamiento teniendo un 0 y 1 respectivamente, de manera similar sucede en la columna *Por_dia*."
      ],
      "metadata": {
        "id": "O5VWDkKc6_1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Normalizacion**\n",
        "\n",
        "Podemos normalizar cada muestra\n",
        "del dataset usando la normalización vectorial:\n",
        "\n",
        "$$\\widehat{X}_{i,j}=\\frac{X_{i,j}}{||X_i||_p}$$"
      ],
      "metadata": {
        "id": "cd3oHI3q62wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql.types import FloatType\n",
        "#Nomazlizar elementos aleatorios \n",
        "def normalizar(x):\n",
        "    #Vector para normalizar\n",
        "    listaCuadrada=x.map(lambda xi:xi*xi)\n",
        "    total=listaCuadrada.sum()\n",
        "    val=math.sqrt(total)\n",
        "    #Escalonar vector\n",
        "    lista= x.map(lambda xi :(xi/val))\n",
        "    return lista \n",
        "\n",
        "#Creamos datos\n",
        "df = spark.createDataFrame([ (1, 'Aldo',12560,45,\"True\"),\n",
        "                             (2, 'Benzema',42560,90,\"False\"),\n",
        "                             (3, 'Cesar',31285,81,\"True\"),\n",
        "                             (4, 'Doris',25285,50,\"True\"),\n",
        "                             (5, 'Etson',32285,55,\"False\"),\n",
        "                             (6, 'Fausto',18085,61,\"True\"),\n",
        "                             (7, 'Gilberto',52185,70,\"False\"),\n",
        "                             (8, 'Humberto',10345,46,\"False\")\n",
        "                           ], [\"id_usuario\", \"Nombre\",\"Ingresos\",\"Por_dia\",\"Morocidad\"])\n",
        "\n",
        "#Mostrar datos iniciales\n",
        "print(\"Data :\")\n",
        "df.show(8)\n",
        "\n",
        "\n",
        "lista = df.select('Ingresos').rdd.map(lambda row : row[0]).collect()\n",
        "Vector = sc.parallelize(lista,4)\n",
        "VectorEscalonada=normalizar(Vector)\n",
        "a=VectorEscalonada.collect()\n",
        "df2 = spark.createDataFrame(a, FloatType())\n",
        "df2=df2.withColumnRenamed(\"value\", \"Ingresos_Normalizado\")\n",
        "print(\"Columna Ingresos Normalizado :\")\n",
        "#Mostrar la columna\n",
        "df2.show()\n"
      ],
      "metadata": {
        "id": "Kmttv-2tJ1AU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b99dcb97-31bf-4747-e86f-83ca811e07d6"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data :\n",
            "+----------+--------+--------+-------+---------+\n",
            "|id_usuario|  Nombre|Ingresos|Por_dia|Morocidad|\n",
            "+----------+--------+--------+-------+---------+\n",
            "|         1|    Aldo|   12560|     45|     True|\n",
            "|         2| Benzema|   42560|     90|    False|\n",
            "|         3|   Cesar|   31285|     81|     True|\n",
            "|         4|   Doris|   25285|     50|     True|\n",
            "|         5|   Etson|   32285|     55|    False|\n",
            "|         6|  Fausto|   18085|     61|     True|\n",
            "|         7|Gilberto|   52185|     70|    False|\n",
            "|         8|Humberto|   10345|     46|    False|\n",
            "+----------+--------+--------+-------+---------+\n",
            "\n",
            "Columna Ingresos Normalizado :\n",
            "+--------------------+\n",
            "|Ingresos_Normalizado|\n",
            "+--------------------+\n",
            "|          0.14233384|\n",
            "|          0.48230317|\n",
            "|          0.35453135|\n",
            "|           0.2865375|\n",
            "|          0.36586368|\n",
            "|          0.20494485|\n",
            "|          0.59137666|\n",
            "|          0.11723276|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Binarizacion**\n",
        "Consiste en transformar las cadenas de texto que tienen valores de string (True y False) a valores numericos, 0 para False y 1 para True"
      ],
      "metadata": {
        "id": "EoZh1szDMvg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from functools import reduce"
      ],
      "metadata": {
        "id": "zVDQBk9bM138"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos el data a usar\n",
        "df = spark.createDataFrame([ (1, 'Aldo',12560,45,\"True\"),\n",
        "                             (2, 'Benzema',42560,90,\"False\"),\n",
        "                             (3, 'Cesar',31285,81,\"True\"),\n",
        "                             (4, 'Doris',25285,50,\"True\"),\n",
        "                             (5, 'Etson',32285,55,\"False\"),\n",
        "                             (6, 'Fausto',18085,61,\"True\"),\n",
        "                             (7, 'Gilberto',52185,70,\"False\"),\n",
        "                             (8, 'Humberto',10345,46,\"False\")\n",
        "                           ], [\"id_usuario\", \"Nombre\",\"Ingresos\",\"Por_dia\",\"Morocidad\"])\n",
        "\n",
        "#Mostramos los datos\n",
        "print(\"Data :\")\n",
        "df.show()\n",
        "\n",
        "#Aqui podemos elegir en que columnas aplicar la binarizacion\n",
        "cols = ['Morocidad']\n",
        "\n",
        "#Aplicar la binarizacion al dataframe spark\n",
        "df_reduced = reduce(lambda df, c: df.withColumn(c, F.when(df[c] == 'False', 0.0).otherwise(1.0)), cols, df)\n",
        "\n",
        "#Mostrar columna binarizada\n",
        "print(\"Data + Columna binarizada:\")\n",
        "df_reduced.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNkS2v_hM2rj",
        "outputId": "f1e516f8-432a-4129-b6df-3730701fc1bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data :\n",
            "+----------+--------+--------+-------+---------+\n",
            "|id_usuario|  Nombre|Ingresos|Por_dia|Morocidad|\n",
            "+----------+--------+--------+-------+---------+\n",
            "|         1|    Aldo|   12560|     45|     True|\n",
            "|         2| Benzema|   42560|     90|    False|\n",
            "|         3|   Cesar|   31285|     81|     True|\n",
            "|         4|   Doris|   25285|     50|     True|\n",
            "|         5|   Etson|   32285|     55|    False|\n",
            "|         6|  Fausto|   18085|     61|     True|\n",
            "|         7|Gilberto|   52185|     70|    False|\n",
            "|         8|Humberto|   10345|     46|    False|\n",
            "+----------+--------+--------+-------+---------+\n",
            "\n",
            "Data + Columna binarizada:\n",
            "+----------+--------+--------+-------+---------+\n",
            "|id_usuario|  Nombre|Ingresos|Por_dia|Morocidad|\n",
            "+----------+--------+--------+-------+---------+\n",
            "|         1|    Aldo|   12560|     45|      1.0|\n",
            "|         2| Benzema|   42560|     90|      0.0|\n",
            "|         3|   Cesar|   31285|     81|      1.0|\n",
            "|         4|   Doris|   25285|     50|      1.0|\n",
            "|         5|   Etson|   32285|     55|      0.0|\n",
            "|         6|  Fausto|   18085|     61|      1.0|\n",
            "|         7|Gilberto|   52185|     70|      0.0|\n",
            "|         8|Humberto|   10345|     46|      0.0|\n",
            "+----------+--------+--------+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#**4. Distancia de Minkowski**\n",
        "Como sabemos la distancia de minkowski es la Distancia entre dos vectores numéricos.\n",
        "\n",
        "El parámetro p de la métrica de distancia de Minkowski de **`Pyspark`** representa el orden de la norma. Cuando el orden (p) es 1, representará la Distancia de Manhattan y cuando el orden en la fórmula anterior es 2, representará la Distancia euclidiana.\n",
        "\n",
        "Cuando nuestros objetos están representados en el espacio Minkowski, medimos la similitud entre ellos a través de la *p-Norma* definida por:\n",
        "\n",
        "$$d(x,y,p) = (\\sum_{i=1}^{n}{|x_i - y_i|^p})^{1/p}$$      $$p≥1$$\n",
        "\n",
        "la norma $p$ siempre puede tomar cualquier valor:\n",
        "\n",
        "\n",
        "$$d(x,y,p) = (\\sum_{i=1}^{n}{|x_i - y_i|^p})$$\n",
        "\n",
        "$$d(x,y,\\infty) = \\max(|x_1 - y_1|,|x_2 - y_2|, ..., |x_n - y_n|)^p$$\n",
        "\n",
        "$$d(x,y,\\infty) = \\min(|x_1 - y_1|,|x_2 - y_2|, ..., |x_n - y_n|)^p$$"
      ],
      "metadata": {
        "id": "kbD1EbuGvx5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Creemos una función rNorm que toma como parámetro y devuelve una función que calcula el pNorm\n",
        "def pNorm(p):\n",
        "    def Dist(x,y):\n",
        "        return np.power(np.power(np.abs(x-y),p).sum(),1/float(p))\n",
        "    return Dist\n",
        "# Creemos un RDD con valores numéricos.\n",
        "np.random.seed(50)\n",
        "num_p = sc.parallelize(enumerate(np.random.random(size=(10,100))))"
      ],
      "metadata": {
        "id": "HJvzO747tavS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p91RbGH10KVy",
        "outputId": "117ffecc-d02a-4b41-8a6c-cd1de3b43739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui mostramos la formula del minkowski tiene un parecido a la formula euclidian lo que diferencia es su norma ya que el monkwoski puede aceptar valores P y el euclian solo valor 2\n"
      ],
      "metadata": {
        "id": "XlgDE3Rd2C5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Formula Minswoski\n",
        "dat_p = num_p.cartesian(num_p)\n",
        "dato_p = dat_p.map(lambda x: ((x[0][0],x[1][0]), (x[0][1],x[1][1])))\n",
        "#minwoski el valor p tendra diferentes valores \n",
        "p = 5\n",
        "#p = 6\n",
        "# p = 7\n",
        "Minkow = pNorm(p)\n",
        "dist = dato_p.map(lambda x: (x[0], Minkow(x[1][0],x[1][1])))\n",
        "soluc = dist.map(lambda x: x[1])\n",
        "minv, maxv, meanv = soluc.min(), soluc.max(), soluc.mean()\n",
        "print('minimo valor minkowski: ',minv)\n",
        "print('maximo valor minkowski: ',maxv)\n",
        "print('Media de los valores minkowski: ',meanv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rHro22QzY5t",
        "outputId": "a580e7fb-40d1-454c-c483-041685449cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimo valor minkowski:  0.0\n",
            "maximo valor minkowski:  1.444514010466728\n",
            "Media de los valores minkowski:  1.2070128755149965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. Distancia Jaccard**\n",
        "\n",
        "La distancia de Jaccard ( $I_J$ ) o coeficiente de Jaccard ( $I_J$ ) mide el grado de similitud entre dos conjuntos, sea cual sea el tipo de elementos.\n",
        "\n",
        "La formulación es la siguiente:\n",
        "\n",
        "$$ J(x,y) = \\frac{\\sum_{i=1}^{n}{x_i == y_i} }{\\sum_{i=1}^{n}{\\max(x_i, y_i}) } $$"
      ],
      "metadata": {
        "id": "XayAlDl1TeUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calcula la distancia Jaccard entre dos vectores binarios.\n",
        "#x, y (np.array): Matriz de enteros binarios x and y.\n",
        "#Returns: J (int): La distancia Jaccard entre x and y.\n",
        "def Jaccard(x,y):\n",
        "    return (x==y).sum()/float( np.maximum(x,y).sum())\n",
        "\n",
        "colores = sc.parallelize(enumerate([['rojo', 'negro', 'azul'],\n",
        "                             ['rosado', 'negro', 'verde'],\n",
        "                             ['rosado', 'amarillo', 'azul'],\n",
        "                             ['rosado', 'negro', 'verde'],\n",
        "                             ['rojo', 'amarillo', 'verde'],\n",
        "                            ]))\n",
        "datos = (colores\n",
        "             .flatMap(lambda x: [((x[0],xi),1) for xi in x[1]])\n",
        "             .reduceByKey(lambda x,y: x)\n",
        "             .map(lambda x: x[0])\n",
        "             )\n",
        "\n",
        "dato = dict((v,k) for k,v in datos.collect())\n",
        "ndato = len(dato)\n",
        "print(dato, ndato)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuTiFYr6UGg_",
        "outputId": "f749f332-9ed0-4c42-beb7-210b1e23555d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rojo': 4, 'negro': 3, 'rosado': 3, 'verde': 3, 'azul': 2, 'amarillo': 4} 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Binarizar el vector categórico usando un diccionario de key\n",
        "#atributos (lista): Lista de atributos de un objeto dado\n",
        "def Bina(atributos,dato):  \n",
        "    array = np.zeros(len(dato))\n",
        "    for atr in atributos:\n",
        "        array[ dato[atr] ] = 1\n",
        "    return array\n",
        "\n",
        "# Convierta datosa formato binario usando key  dict\n",
        "binarizar = colores.map(lambda rec: (rec[0],Bina(rec[1], dato)))\n",
        "binarizar.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9WvBBV3VZul",
        "outputId": "771a56d0-d91e-4b08-9d3a-7aad8a27ad8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, array([0., 0., 1., 1., 1., 0.])),\n",
              " (1, array([0., 0., 0., 1., 0., 0.])),\n",
              " (2, array([0., 0., 1., 1., 1., 0.])),\n",
              " (3, array([0., 0., 0., 1., 0., 0.])),\n",
              " (4, array([0., 0., 0., 1., 1., 0.]))]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adquirir dentro de los  PySpark para hallar produto cartesiano \n",
        "Binario = binarizar.cartesian(binarizar)\n",
        "# Aplicar un mapa para transformar nuestro RDD en un RDD de tupla ((id1, id2), (vector1, vector2))\n",
        "# use el comando take (1) e imprima el resultado para verificar el formato RDD actual\n",
        "Binario_par = Binario.map(lambda x: ((x[0][0],x[1][0]), (x[0][1],x[1][1])))\n",
        "#Aplicar un mapa para calcular la distancia de Jaccard entre pares\n",
        "jacRDD = Binario_par.map(lambda x: (x[0], Jaccard(x[1][0],x[1][1])))\n",
        "#calcular min, max, mean\n",
        "statJRDD = jacRDD.map(lambda x: x[1])\n",
        "Jmin, Jmax, Jmean = statJRDD.min(), statJRDD.max(), statJRDD.mean()\n",
        "print (\"\\t\\tMin\\tMax\\tMedia\")\n",
        "print (\"Jaccard:\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format( Jmin, Jmax, Jmean ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4xKMx0EdVJ9",
        "outputId": "b4ad144c-d2d7-4d99-9d3d-c3950bc72419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\tMin\tMax\tMean\n",
            "Jaccard:\t1.33\t6.00\t2.49\n"
          ]
        }
      ]
    }
  ]
}